{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading\n",
    "#loading anamoly method and pipeline model \n",
    "iso, model = joblib.load('data_sample_data_v1.pkl')\n",
    "#loading dataset\n",
    "dataset=joblib.load('datasetnew.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22586, 21)\n",
      "(22586,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dataset.iloc[:,0:21])\n",
    "y = np.array(dataset.went_on_backorder)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21403, 21)\n",
      "(21403,)\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "#reducing outliers on training set: Anomaly detection methon\n",
    "iso_outliers = iso.predict(X)==-1\n",
    "\n",
    "X_iso_train = X[~iso_outliers]\n",
    "y_iso_train = y[~iso_outliers]\n",
    "print(X_iso_train.shape)\n",
    "print(y_iso_train.shape)\n",
    "#pipeline model to train full sample training data set\n",
    "model_s=model.fit(X_iso_train, y_iso_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_s.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "\n",
    "joblib.dump(model_s, 'model_s.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (**follow** the steps similar to Part I)\n",
    "* **If you have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3518906</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3519799</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3454288</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3320245</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3438836</td>\n",
       "      <td>155.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  3518906           2.0        8.0             0.0               3.0   \n",
       "1  3519799           2.0        8.0             0.0               0.0   \n",
       "2  3454288           3.0        9.0             0.0               0.0   \n",
       "3  3320245          21.0        8.0             0.0               0.0   \n",
       "4  3438836         155.0        9.0             0.0               0.0   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0               6.0              10.0            1.0            4.0   \n",
       "1               0.0               0.0            0.0            0.0   \n",
       "2               0.0               1.0            0.0            1.0   \n",
       "3               0.0               0.0            0.0            0.0   \n",
       "4               0.0               0.0           10.0           45.0   \n",
       "\n",
       "   sales_6_month  ...  pieces_past_due  perf_6_month_avg perf_12_month_avg  \\\n",
       "0            6.0  ...              0.0              0.66              0.66   \n",
       "1            0.0  ...              0.0              0.54              0.71   \n",
       "2            1.0  ...              0.0              0.70              0.77   \n",
       "3           15.0  ...              0.0              0.84              0.80   \n",
       "4          119.0  ...              0.0              0.94              0.83   \n",
       "\n",
       "   local_bo_qty  deck_risk  oe_constraint  ppap_risk stop_auto_buy rev_stop  \\\n",
       "0           0.0         No             No         No           Yes       No   \n",
       "1           0.0         No             No         No           Yes       No   \n",
       "2           0.0         No             No         No           Yes       No   \n",
       "3           0.0         No             No         No           Yes       No   \n",
       "4           0.0         No             No        Yes           Yes       No   \n",
       "\n",
       "  went_on_backorder  \n",
       "0                No  \n",
       "1                No  \n",
       "2                No  \n",
       "3                No  \n",
       "4                No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET = 'Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "dataset = pd.read_csv(DATASET, sep=',').sample(frac = 1).reset_index(drop=True)\n",
    "dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "potential_issue: ['No' 'Yes' nan]\n",
      "deck_risk: ['No' 'Yes' nan]\n",
      "oe_constraint ['No' 'Yes' nan]\n",
      "ppap_risk ['No' 'Yes' nan]\n",
      "stop_auto_buy ['Yes' 'No' nan]\n",
      "rev_stop ['No' 'Yes' nan]\n",
      "went_on_backorder: ['No' 'Yes' nan]\n",
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing: using same steps used in part1 except undersampling\n",
    "# Dropped column: sku\n",
    "dataset= dataset.iloc[:,1:23]\n",
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E102)\n",
    "# ----------------------------------\n",
    "\n",
    "#finding wheather a column contain only yes/no\n",
    "\n",
    "print(\"potential_issue:\", dataset.potential_issue.unique())\n",
    "print(\"deck_risk:\", dataset.deck_risk.unique())\n",
    "print(\"oe_constraint\", dataset.oe_constraint.unique())\n",
    "print(\"ppap_risk\", dataset.ppap_risk.unique())\n",
    "print(\"stop_auto_buy\", dataset.stop_auto_buy.unique())\n",
    "print(\"rev_stop\", dataset.rev_stop.unique())\n",
    "print(\"went_on_backorder:\", dataset.went_on_backorder.unique())\n",
    "\n",
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)\n",
    "    \n",
    "cat = dataset.select_dtypes(include = ['object']).columns\n",
    "for col in cat:\n",
    "    dataset[col].replace({'No': 0, 'Yes': 1}, inplace=True)\n",
    "    dataset[col] = dataset[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242075, 21)\n",
      "(242075,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset.isnull().sum()\n",
    "dataset['lead_time'].fillna(dataset['lead_time'].mode()[0], inplace=True) #replacing null values of lead time column with mode\n",
    "dataset=dataset.dropna()\n",
    "test_X = np.array(dataset.iloc[:,0:21])\n",
    "test_y = np.array(dataset.went_on_backorder)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95    239387\n",
      "           1       0.09      0.80      0.16      2688\n",
      "\n",
      "    accuracy                           0.90    242075\n",
      "   macro avg       0.54      0.85      0.55    242075\n",
      "weighted avg       0.99      0.90      0.94    242075\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216851</td>\n",
       "      <td>22536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551</td>\n",
       "      <td>2137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "0  216851  22536\n",
       "1     551   2137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "#scenario 1: without any outliers reduction on test data\n",
    "\n",
    "predicted_y1 = model_s.predict(test_X)\n",
    "print(classification_report(test_y, predicted_y1))\n",
    "pd.DataFrame(confusion_matrix(test_y, predicted_y1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229635, 21)\n",
      "(229635,)\n"
     ]
    }
   ],
   "source": [
    "#Now, I have removed outliers from the test dataset\n",
    "\n",
    "#outlier reduction on test set\n",
    "\n",
    "iso_outliers = iso.predict(test_X)==-1\n",
    "\n",
    "X_iso_test = test_X[~iso_outliers]\n",
    "y_iso_test = test_y[~iso_outliers]\n",
    "print(X_iso_test.shape)\n",
    "print(y_iso_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95    227091\n",
      "           1       0.09      0.80      0.15      2544\n",
      "\n",
      "    accuracy                           0.90    229635\n",
      "   macro avg       0.54      0.85      0.55    229635\n",
      "weighted avg       0.99      0.90      0.94    229635\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205295</td>\n",
       "      <td>21796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "0  205295  21796\n",
       "1     519   2025"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scenario 2: Outlier removed from train and test dataset\n",
    "\n",
    "predicted_y2 = model_s.predict(X_iso_test)\n",
    "print(classification_report(y_iso_test, predicted_y2))\n",
    "pd.DataFrame(confusion_matrix(y_iso_test, predicted_y2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95    239387\n",
      "           1       0.09      0.79      0.16      2688\n",
      "\n",
      "    accuracy                           0.90    242075\n",
      "   macro avg       0.54      0.85      0.55    242075\n",
      "weighted avg       0.99      0.90      0.94    242075\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216802</td>\n",
       "      <td>22585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>557</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "0  216802  22585\n",
       "1     557   2131"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scenario 3: No outlier reduction on train as well as test data\n",
    "model_s_out=model.fit(X, y)\n",
    "\n",
    "predicted_y3 = model_s_out.predict(test_X)\n",
    "print(classification_report(test_y, predicted_y3))\n",
    "pd.DataFrame(confusion_matrix(test_y, predicted_y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95    227091\n",
      "           1       0.08      0.79      0.15      2544\n",
      "\n",
      "    accuracy                           0.90    229635\n",
      "   macro avg       0.54      0.85      0.55    229635\n",
      "weighted avg       0.99      0.90      0.94    229635\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205240</td>\n",
       "      <td>21851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "0  205240  21851\n",
       "1     525   2019"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scenario 4: No outlier reduction on training set but inlier test data\n",
    "\n",
    "predicted_y4 = model_s_out.predict(X_iso_test)\n",
    "print(classification_report(y_iso_test, predicted_y4))\n",
    "pd.DataFrame(confusion_matrix(y_iso_test, predicted_y4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688\n",
      "239387\n",
      "backorder ratio: 2688 / 242075 = 0.01110399669523908\n"
     ]
    }
   ],
   "source": [
    "#checking test dataset wheather there is any class imbalance problem \n",
    "\n",
    "print(np.sum(dataset['went_on_backorder']==1))\n",
    "print(np.sum(dataset['went_on_backorder']==0))\n",
    "num_backorder = np.sum(dataset['went_on_backorder']==1)\n",
    "print('backorder ratio:', num_backorder, '/', len(dataset), '=', num_backorder / len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    ">>> ObservationS on test dataset: The test dataset suffers from a severe class imbalance problem. One class is having 2688 data points and another class having 239387 data points.\n",
    "\n",
    "\n",
    "\n",
    ">>> With vs without outliers on test dataset:\n",
    "Firstly, we should not remove outliers on the test data set because it sometime signify rare class or some important and sensitive data. If we remove outliers from the test data it may end up with removing some vital data points. \n",
    "Here, I experimented with four different scenarios:\n",
    "scenario 1: Here, I used test data set without any outlier reduction, inlier train data.\n",
    "scenario 2: Outliers were removed from the test dataset as well as train data.\n",
    "scenario 3: No outlier reduction on train as well as test dataset.\n",
    "scenario 4: Removed outlier fron the test data but train data with outliers.\n",
    "From the classification report of all four cases it was clearly observable that the results were almost similar. In all the cases overall accuracy was 90% and for class 0, the precision was 1, which indicated that 100% of the cases the model correctly identified as belonging to class 0 were observed. Recall for class 0 were 0.89-0.90, which indicates that 89%-90% of all instances of class 0 were correctly predicted by the model. The harmonic mean of precision and recall for class 0 was 0.95, and this value represents the class 0 f1-score. There were approx range of 220000-230000 occurrences of class 0 in the dataset, as shown by the class 0 support of range 220000-230000.\n",
    "The precision for class 1 was 0.08(approx.), meaning that 8% of the occurrences the model predicted to be in class 1 were in fact in class 1. Recall for class 1 is 0.79, which indicated that 79% of all occurrences that belong to class 1 were correctly predicted by the model. The harmonic mean of precision and recall for class 1 was 0.15, and this value is used to calculate the class 1 f1-score. There were 2600(approx.) occurrences of class 1 in the dataset, as shown by the value of 2600 for the support for class 1. \n",
    "All the models accuracy was 0.90, which indicates that it correctly identified the class for 90% of the dataset's cases. The unweighted average of the f1 scores for both classes, known as the macro-averaged f1-score, is 0.55. The average of the f1-scores for the two classes, weighted by the quantity of occurrences in each class, is 0.94. It is clear from all four classification report that for class 1 precision value was very low(8%) as a result f1 score for class 1 was .15. The model did not perform up to the mark.\n",
    "\n",
    "\n",
    "\n",
    ">>> Class imbalance: This test data set suffers from severe class imbalance problem. Majority class having 239387 data points and the minority class having 2688 data points. As a result, it can simply predict all instances as majority class which is '0' here, achieving an accuracy of 95%. However, this model has poor performance on the minority class ('1'), with low precision, recall, and F1-score.\n",
    "A quick and efficient method for balancing the class distribution in unbalanced datasets is random sampling. It includes reducing the size of the majority class by randomly eliminating instances from it. The drawback is that it could exclude important data, particularly if the collection is already limited.\n",
    "On the other hand, random undersampling may not be necessary or perhaps lead to an over-correction if the data is only somewhat uneven as opposed to being extremely skewed. Such circumstances can call for a more subtle undersampling strategy. By reproducing its instances or employing techniques for creating synthetic data, we may, for instance, oversample the minority class rather than undersample the majority class. Alternately, to establish a better balance between the classes, we may combine undersampling and oversampling.\n",
    "The effectiveness of the model can be significantly impacted by the technique of undersampling that is selected. Some undersampling techniques can lessen the class imbalance while preserving the structure and variety of the dominant class, improving performance. Other approaches, however, can miss important data and produce less accurate results. To choose the optimal strategy for the particular dataset and problem, it is crucial to test out several undersampling techniques and evaluate their effectiveness.\n",
    "\n",
    "\n",
    "\n",
    ">>> SUMMARY: My model is robust to outliers. In this part I experimented with four scenarios but there are no significant differences among their result. \n",
    "\n",
    "\n",
    "\n",
    ">>> I decided 'best' performance as per classification report. At first, I considered confusion matrix to check exactly how many data points were classified properly (true positive, true negative) and how many are missclassified (false positive, false negative) then classification report (explained above). As, I got almost similar results in all four scenarios. From the confusion matrix of scenario 1 on 242075 datapoints true negative was 216851 (no. of negative data points classified correctly), true positive was 2137 (no. of positive data points classified correctly), false nagative was 551 (no. of positive datapoints but predicted negative) and false positive was 22536 (no. of negative datapoints but predicted positive). Overall, 90% of the data classified correctly but number of false negative cases were high.\n",
    "In general, accuracy may not be the appropriate statistic for a situation with an imbalance since it may be skewed towards the majority class and fail to accurately reflect the model's performance. It may be better to use metrics like precision, recall, and F1 score.\n",
    "Recall assesses the proportion of real positives among the actual positives as opposed to precision, which assesses the proportion of true positives among the projected positives. The harmonic mean of recall and accuracy, known as the F1 score, strikes a compromise between the two metrics.\n",
    "The cost of false negatives (not forecasting a backorder when it occurs) may be larger than the cost of false positives (predicting a backorder when it doesn't occur) in the situation of backorder prediction, when the objective is to determine which goods may be out of stock and require restocking. This is because a false positive might lead to excess inventory or unnecessary backorders, whereas a false negative could lead to missed sales and unhappy consumers.\n",
    "The capacity of the model to accurately detect all instances of backorders, even if it produces some false positives, is captured by recall, making it a more significant statistic to optimize for in this situation. Precision should be taken into account as well, though, since it's critical to reduce the amount of false positives to prevent irrational restocking or inventory expenses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    ">>> Summary for the management (non-technical audience):\n",
    "Briefly, according to the classification report, the model can correctly predict 90% of positive predictions (recall), a 100% the accuracy of positive predictions (precision) rate, and a 90% accuracy rate for predicting non-backorder goods. The model may incorrectly forecast many things as being backordered, as seen by the fact that the accuracy of predicting backorder items is just 8%. The algorithm may overlook some backorder products, as seen by the recall of 79% when predicting backorder items.\n",
    "\n",
    "Nevertheless, I think the model may still offer insightful information and aid in streamlining the inventory control and replenishment procedures. The model performed well in terms of accuracy, precision, and recall after being trained on a sizable dataset. However, it's crucial to bear in mind the trade-off between false positives and false negatives, as well as to take into account the costs and hazards connected with each sort of error.\n",
    "\n",
    "> Should they use your model?\n",
    "Yes, they should use my model but it totaly depends on the context that what they are looking for.\n",
    "Utilizing the model will rely on the particular requirements and trade-offs of the firm. The model might not be the ideal option if reducing the amount of false positives (things predicted as backorder but are not) is important. On the other side, the approach could still be helpful if reducing false negatives (backorder goods that are overlooked) is more crucial.\n",
    "\n",
    "> What are the constraints/limitations of your model?\n",
    "The model has several restrictions and limitations, which should be noted. The main emphasis of the issue at hand is backorder products, therefore specifically it could not perform well in this area. Additionally, the model's poor accuracy in forecasting backorder products may cause a considerable quantity of unneeded inventory to accumulate, which might be detrimental to the organization.\n",
    "\n",
    "\n",
    "> Does the model do a good job with predicting backorder items?\n",
    "To put it another way, the model is effective at locating non-backorder products but less effective at anticipating backorder items. 90% of backorder goods are successfully predicted, however many non-backorder items may be incorrectly predicted as backorder. The model could also overlook certain backordered products. The unique requirements and trade-offs of the business should be taken into account when making the choice to employ the model, and the possible risks and advantages of doing so should be carefully addressed.\n",
    "\n",
    "\n",
    "Overall, the model should be used in conjunction with other tools and techniques for inventory management and spotting possible backorders. By applying the model, there is a potential that certain backorder goods might be missed or mostly incorrectly predicted as such. The management group must choose which trade-off is most palatable for the company."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
